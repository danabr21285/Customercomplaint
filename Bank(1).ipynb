{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01c149d-6b0c-4eb7-931f-cc22c49ebbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI-Powered Customer Complaint Classification\n",
    "\n",
    "### By:Dana Brooks\n",
    "\n",
    "---\n",
    "\n",
    "**Project Goal:** This notebook documents the end-to-end process of building a machine learning model to automatically classify customer mortgage complaints into one of 22 distinct categories.\n",
    "\n",
    "**Process:** The project involves data cleaning, exploratory data analysis (EDA), natural language processing (NLP) with TF-IDF, and an iterative modeling process to find the most effective solution.\n",
    "\n",
    "**Result:** The final **Logistic Regression model** successfully categorizes complaints with **52% accuracy**, a result that is over **11 times better than random chance**, demonstrating a strong and reliable baseline for this business problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c68496-5792-4dff-b5fc-9dacd5deb41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a4037-61eb-4a36-b97f-0c76f10b4c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83358f08-8725-4900-b875-e0d9b78ed385",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'upload dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a399dbfb-637e-425c-ac3e-3cbba1945948",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First 5 rows:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e865a04-d63c-4a16-9ccb-231727b500ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDataset shape:\")\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9284a98e-b1e9-4635-9976-069fc377887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDataset info:\")\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc6c38f-91e2-4586-9736-648c4dffeb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df[['consumer_complaint_code', 'consumer_complaint_narrative']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74689343-938a-46bb-aacb-ee19985935ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.columns = ['issue', 'complaint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306ba21b-16e1-43dd-98ad-39580da1f917",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"New focused DataFrame:\")\n",
    "print(df_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a84b5f8-a3fa-471f-b3c2-ed290f2b8336",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDistribution of complaints per issue:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2d802b-aaa2-47e4-a7b7-1b3fe72511e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clean['issue'].value_counts().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23740e69-fcef-4483-aab5-71e791926ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.countplot(y='issue', data=df_clean, order=df_clean['issue'].value_counts().index[:20])\n",
    "plt.title('Top 20 Mortgage Complaint Issues')\n",
    "plt.xlabel('Number of Complaints')\n",
    "plt.ylabel('Issue Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcc5b62-1c9f-4b72-9863-2c573f3f8dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccf1fc0-cee6-43c5-845b-969652189442",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624b7290-477c-43a2-b5ab-34efe4e1aec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2292fa-fb88-4193-9fd2-eed9e3038227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove punctuation\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "    # 3. Tokenize (split text into words)\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # 4. Remove stop words and lemmatize\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "\n",
    "    # 5. Join words back into a single string\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "print(\"Function defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac93381c-ce04-48ee-80af-9355b065e3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to our 'complaint' column\n",
    "print(\"Cleaning text data... this may take a moment.\")\n",
    "df_clean['cleaned_complaint'] = df_clean['complaint'].apply(preprocess_text)\n",
    "print(\"Cleaning complete!\")\n",
    "\n",
    "# View the results\n",
    "print(df_clean[['complaint', 'cleaned_complaint']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b9fa0a-7972-4cae-9bd2-4da08e2f7d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# First, get the counts of each issue\n",
    "issue_counts = df_clean['issue'].value_counts()\n",
    "\n",
    "# Identify the issues that have more than one complaint\n",
    "issues_to_keep = issue_counts[issue_counts > 1].index\n",
    "\n",
    "# Filter the DataFrame to only include the issues we want to keep\n",
    "df_clean_filtered = df_clean[df_clean['issue'].isin(issues_to_keep)]\n",
    "\n",
    "print(f\"Original number of rows: {len(df_clean)}\")\n",
    "print(f\"Number of rows after filtering rare categories: {len(df_clean_filtered)}\")\n",
    "\n",
    "\n",
    "# --- Now, proceed with the split using the FILTERED data ---\n",
    "X = df_clean_filtered['cleaned_complaint']\n",
    "y = df_clean_filtered['issue']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "# --- Initialize and run the TF-IDF Vectorizer ---\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(\"\\nTF-IDF vectors created successfully on filtered data.\")\n",
    "print(f\"Shape of training vectors: {X_train_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5042f517-8d70-4bfa-a775-e2174935baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 1. Initialize the Model\n",
    "# We use random_state for reproducibility\n",
    "model = LogisticRegression(random_state=42)\n",
    "\n",
    "# 2. Train the Model\n",
    "print(\"Training the model...\")\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# 3. Make Predictions on the Test Data\n",
    "print(\"\\nMaking predictions on the test set...\")\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# 4. Evaluate the Model's Performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
